{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanfu/miniconda3/envs/ml/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "from itertools import count\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "\n",
    "import stockcore.data as scdata\n",
    "import stockcore.models as scmodels\n",
    "import stockcore.environment as scenv\n",
    "import stockcore.utils as scutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = scutils.setup_mpl()\n",
    "device = scutils.get_device()\n",
    "\n",
    "if is_ipython:\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37305, 7)\n",
      "(37305, 7)\n",
      "                  date           unix     open     high      low    close  \\\n",
      "0  2020-08-18 07:00:00  1597734000000  12278.7  12289.3  12246.5  12284.4   \n",
      "1  2020-08-18 08:00:00  1597737600000  12283.5  12306.1  12265.5  12265.8   \n",
      "2  2020-08-18 09:00:00  1597741200000  12265.0  12273.9  12231.3  12235.2   \n",
      "3  2020-08-18 10:00:00  1597744800000  12234.8  12313.2  12197.9  12295.4   \n",
      "4  2020-08-18 11:00:00  1597748400000  12293.7  12298.8  12180.4  12189.4   \n",
      "\n",
      "       volume  \n",
      "0  167.048618  \n",
      "1  148.145490  \n",
      "2  155.735895  \n",
      "3  206.575700  \n",
      "4  210.599326  \n",
      "                  date           unix    open    high     low   close  \\\n",
      "0  2020-08-18 07:00:00  1597734000000  430.00  435.00  410.00  430.30   \n",
      "1  2020-08-18 08:00:00  1597737600000  430.27  431.79  430.27  430.80   \n",
      "2  2020-08-18 09:00:00  1597741200000  430.86  431.13  428.71  429.35   \n",
      "3  2020-08-18 10:00:00  1597744800000  429.75  432.69  428.59  431.90   \n",
      "4  2020-08-18 11:00:00  1597748400000  432.09  432.89  426.99  427.45   \n",
      "\n",
      "        volume  \n",
      "0   487.154463  \n",
      "1   454.176153  \n",
      "2  1183.710884  \n",
      "3  1686.183227  \n",
      "4  1980.692724  \n"
     ]
    }
   ],
   "source": [
    "# read csv as pandas dataframe\n",
    "btc_usd_df = pd.read_csv('btc_usd.csv')\n",
    "eth_usd_df = pd.read_csv('eth_usd.csv')\n",
    "print(btc_usd_df.shape)\n",
    "print(eth_usd_df.shape)\n",
    "print(btc_usd_df.head(5))\n",
    "print(eth_usd_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37305, 12)\n",
      "(37305, 12)\n",
      "                  date           unix     open     high      low    close  \\\n",
      "0  2020-08-18 07:00:00  1597734000000  12278.7  12289.3  12246.5  12284.4   \n",
      "1  2020-08-18 08:00:00  1597737600000  12283.5  12306.1  12265.5  12265.8   \n",
      "2  2020-08-18 09:00:00  1597741200000  12265.0  12273.9  12231.3  12235.2   \n",
      "3  2020-08-18 10:00:00  1597744800000  12234.8  12313.2  12197.9  12295.4   \n",
      "4  2020-08-18 11:00:00  1597748400000  12293.7  12298.8  12180.4  12189.4   \n",
      "\n",
      "       volume  feature_close  feature_open  feature_high  feature_low  \\\n",
      "0  167.048618            NaN      0.999536      1.000399     0.996915   \n",
      "1  148.145490      -0.001514      1.001443      1.003286     0.999976   \n",
      "2  155.735895      -0.002495      1.002436      1.003163     0.999681   \n",
      "3  206.575700       0.004920      0.995071      1.001448     0.992070   \n",
      "4  210.599326      -0.008621      1.008557      1.008975     0.999262   \n",
      "\n",
      "   feature_volume  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4             NaN  \n",
      "                  date           unix    open    high     low   close  \\\n",
      "0  2020-08-18 07:00:00  1597734000000  430.00  435.00  410.00  430.30   \n",
      "1  2020-08-18 08:00:00  1597737600000  430.27  431.79  430.27  430.80   \n",
      "2  2020-08-18 09:00:00  1597741200000  430.86  431.13  428.71  429.35   \n",
      "3  2020-08-18 10:00:00  1597744800000  429.75  432.69  428.59  431.90   \n",
      "4  2020-08-18 11:00:00  1597748400000  432.09  432.89  426.99  427.45   \n",
      "\n",
      "        volume  feature_close  feature_open  feature_high  feature_low  \\\n",
      "0   487.154463            NaN      0.999303      1.010923     0.952824   \n",
      "1   454.176153       0.001162      0.998770      1.002298     0.998770   \n",
      "2  1183.710884      -0.003366      1.003517      1.004146     0.998509   \n",
      "3  1686.183227       0.005939      0.995022      1.001829     0.992336   \n",
      "4  1980.692724      -0.010303      1.010855      1.012727     0.998924   \n",
      "\n",
      "   feature_volume  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4             NaN  \n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "btc_usd_df = scdata.data_preprocess(btc_usd_df, dropna=False)\n",
    "eth_usd_df = scdata.data_preprocess(eth_usd_df, dropna=False)\n",
    "print(btc_usd_df.shape)\n",
    "print(eth_usd_df.shape)\n",
    "print(btc_usd_df.head(5))\n",
    "print(eth_usd_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37305, 24)\n",
      "          date_btc_usd   unix_btc_usd  open_btc_usd  high_btc_usd  \\\n",
      "0  2020-08-18 07:00:00  1597734000000       12278.7       12289.3   \n",
      "\n",
      "   low_btc_usd  close_btc_usd  volume_btc_usd  feature_close_btc_usd  \\\n",
      "0      12246.5        12284.4      167.048618                    NaN   \n",
      "\n",
      "   feature_open_btc_usd  feature_high_btc_usd  ...  open_eth_usd  \\\n",
      "0              0.999536              1.000399  ...         430.0   \n",
      "\n",
      "   high_eth_usd low_eth_usd  close_eth_usd  volume_eth_usd  \\\n",
      "0         435.0       410.0          430.3      487.154463   \n",
      "\n",
      "   feature_close_eth_usd  feature_open_eth_usd  feature_high_eth_usd  \\\n",
      "0                    NaN              0.999303              1.010923   \n",
      "\n",
      "   feature_low_eth_usd  feature_volume_eth_usd  \n",
      "0             0.952824                     NaN  \n",
      "\n",
      "[1 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge two dataframes\n",
    "btc_usd_df.columns += '_btc_usd'\n",
    "eth_usd_df.columns += '_eth_usd'\n",
    "merged_df = pd.concat([btc_usd_df, eth_usd_df], axis=1)\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37305, 22)\n"
     ]
    }
   ],
   "source": [
    "# check if date_btc_usd and date_eth_usd are the same\n",
    "assert (merged_df['date_btc_usd'] == merged_df['date_eth_usd']).all()\n",
    "# check if unixtime_btc_usd and unixtime_eth_usd are the same\n",
    "assert (merged_df['unix_btc_usd'] == merged_df['unix_eth_usd']).all()\n",
    "\n",
    "#  drop the duplicate columns\n",
    "merged_df = merged_df.drop(columns=['date_eth_usd', 'unix_eth_usd'])\n",
    "print(merged_df.shape)\n",
    "# rename the columns\n",
    "merged_df = merged_df.rename(columns={'date_btc_usd': 'date', 'unix_btc_usd': 'unix'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37138, 22)\n"
     ]
    }
   ],
   "source": [
    "merged_df.dropna(inplace=True)\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"TradingEnv\",\n",
    "        name= \"BTC_ETH_USD\",\n",
    "        df = merged_df, # Your dataset with your custom features\n",
    "        positions = [ -1, 0, 1], # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e9e5afe5994f6c8405ac651caae3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 1005.52%   |   Portfolio Return : -99.62%   |   \n"
     ]
    }
   ],
   "source": [
    "# Eatch step, the environment will return 5 inputs  : \"feature_close\", \"feature_open\", \"feature_high\", \"feature_low\", \"feature_volume\"\n",
    "# Run an episode until it ends :\n",
    "done, truncated = False, False\n",
    "observation, info = env.reset()\n",
    "pbar = tqdm(total=len(df))\n",
    "while not done and not truncated:\n",
    "    # Pick a position by its index in your position list (=[-1, 0, 1])....usually something like : position_index = your_policy(observation)\n",
    "    position_index = env.action_space.sample() # At every timestep, pick a random position index from your position list (=[-1, 0, 1])\n",
    "    pbar.update(1)\n",
    "    observation, reward, done, truncated, info = env.step(position_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128  # number of transitions sampled from the replay buffer\n",
    "GAMMA = 0.99  # discount factor\n",
    "EPS_START = 0.9  # starting value of epsilon\n",
    "EPS_END = 0.05  # final value of epsilon\n",
    "EPS_DECAY = 1000  # rate of exponential decay of epsilon, higher means a slower decay\n",
    "TAU = 0.005  # update rate of the target network\n",
    "LR = 1e-4  # learning rate of the `AdamW` optimizer\n",
    "MEMORY_SIZE = 10000  # size of the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n  # Get number of actions from gym action space\n",
    "state, info = env.reset()  # Get the number of state observations\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = scmodels.NaiveDQN(env).to(device)\n",
    "target_net = scmodels.NaiveDQN(env).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = scenv.ReplayMemory(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = scenv.Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                       if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    foo = policy_net(state_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(\n",
    "            non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values,\n",
    "                     expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scutils.is_backend_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def select_action(state: torch.Tensor) -> torch.Tensor:\n",
    "    select_action.steps_done = getattr(select_action, 'steps_done', 0)\n",
    "\n",
    "    eps_threshold = EPS_END + (\n",
    "        (EPS_START - EPS_END) *\n",
    "        math.exp(-1. * select_action.steps_done / EPS_DECAY)\n",
    "    )\n",
    "    \n",
    "    select_action.steps_done += 1\n",
    "\n",
    "    return policy_net.act(state, eps_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b0b9f46b054cd2b8ddb845c11bf21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374c721868a94d0cacc1976c4ddc380f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m      5\u001b[0m     state,\n\u001b[1;32m      6\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m (pbar_step \u001b[38;5;241m:=\u001b[39m tqdm(count(), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df))):\n\u001b[0;32m----> 8\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     10\u001b[0m     reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward], device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i_episode in (pbar_epoch := tqdm(range(num_episodes))):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(\n",
    "        state,\n",
    "        dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in (pbar_step := tqdm(count(), leave=False, total=len(df))):\n",
    "        action = select_action(state).to(device)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(\n",
    "                observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = (\n",
    "                policy_net_state_dict[key] * TAU +\n",
    "                target_net_state_dict[key] * (1 - TAU))\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# torch.save(policy_net.state_dict(), 'model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
